{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Cracking the Code: Unveiling Resume Sorting Strategies"
      ],
      "metadata": {
        "id": "gAbv4onrSm1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### In a world where countless job seekers, myself included, are applying for positions, the sheer volume of resumes overwhelms businesses. Each resumes possesses a distinct structure and content, deviating from any standardized format. Consequently, HR professionals grapple with the daunting task of sifting through these individualistic documents to pinpoint the ideal candidate for a specific job.\n",
        "\n",
        "##### However, this manual review process is both time-intensive and susceptible to oversights, potentially leading to qualified applicants slipping through the cracks. This tutorial aims to highlight how companies harness the power of machine learning to navigate through multiple resumes. By delving into this process, applicants like myself gain insight into the behind-the-scenes journey of their job appcations. Moreover, understanding this can help us enhance our resumes strategically, optimizing our chances of progressing further in the hiring process.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "tTxKZgdsYCbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### To find the best candidate, here's how I assume it works:\n",
        "\n",
        "1. Recuiters review each resume in the database, seeking those that seamlessly align with the job description.\n",
        "2. Then selects resumes that go through a ranking process, where relevance to the job description becomes the defining metric.\n",
        "3. Finally, the key candidate's details such as their name, contact information, and email address are accessed for the next steps of the application process.\n",
        "\n",
        "##### Addressing this use case involves exploring the realm of Natural Language Processing (NLP). This tutorial will highlight how theses systems parse, match, select, and rank resumes from an extensive pool, all predicted on the basis of the job description."
      ],
      "metadata": {
        "id": "LOZAbrtbhjLi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfACJG5_IWH0"
      },
      "outputs": [],
      "source": [
        "! pip install textract\n",
        "! pip install -U nltk\n",
        "! pip install pdfminer3\n",
        "! pip install mammoth\n",
        "! pip install locationtagger"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from pdfminer3.layout import LAParams\n",
        "from pdfminer3.pdfpage import PDFPage\n",
        "from pdfminer3.pdfinterp import PDFResourceManager\n",
        "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer3.converter import TextConverter\n",
        "import io\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import mammoth\n",
        "import locationtagger\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "from sklearn.decomposition import TruncatedSVD\n"
      ],
      "metadata": {
        "id": "mO92oaxQnSWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making directory of Resumes and Job Description\n",
        "directory = '/content/drive/MyDrive/'\n",
        "resume_path = directory + 'Resumes/'\n",
        "jd_path = directory + 'JD/'\n"
      ],
      "metadata": {
        "id": "ivE8sdBjnkok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Next, let’s extract all the text information from the resumes and job description. Let’s write a function that extracts PDF-format resumes. It can also extract the data from tables. Here we use the PDFResourceManager, PDFPageInterpreter, and TextConverter functions, which are deined in pdfminer3."
      ],
      "metadata": {
        "id": "zDhkqB_5oEzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_extractor(path):\n",
        "    r_manager = PDFResourceManager()\n",
        "    output = io.StringIO()\n",
        "    converter = TextConverter(r_manager, output, laparams=LAParams())\n",
        "    p_interpreter = PDFPageInterpreter(r_manager, converter)\n",
        "\n",
        "    with open(path, 'rb') as file:\n",
        "        for page in PDFPage.get_pages(file, caching=True, check_extractable=True):\n",
        "            p_interpreter.process_page(page)\n",
        "            text = output.getvalue()\n",
        "\n",
        "    converter.close()\n",
        "    output.close()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "SRvPxgdYoGvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A function for reading pdf, docx, doc, and txt files\n",
        "def read_files(file_path):\n",
        "    fileTXT = []\n",
        "\n",
        "    # This loop reads all the files in the specified file_path\n",
        "    for filename in os.listdir(file_path):\n",
        "        # For pdf format\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            try:\n",
        "                fileTXT.append(pdf_extractor(file_path + filename))\n",
        "            except Exception:\n",
        "                print('Error reading pdf file: ' + filename)\n",
        "\n",
        "        # For docx format\n",
        "        if filename.endswith(\".docx\"):\n",
        "            try:\n",
        "                with open(file_path + filename, \"rb\") as docx_file:\n",
        "                    result = mammoth.extract_raw_text(docx_file)\n",
        "                    text = result.value\n",
        "                    fileTXT.append(text)\n",
        "            except IOError:\n",
        "                print('Error reading .docx file: ' + filename)\n",
        "\n",
        "        # For doc format\n",
        "        if filename.endswith(\".doc\"):\n",
        "            try:\n",
        "                text = textract.process(file_path + filename).decode('utf-8')\n",
        "                fileTXT.append(text)\n",
        "            except Exception:\n",
        "                print('Error reading .doc file: ' + filename)\n",
        "\n",
        "        # For txt format\n",
        "        if filename.endswith(\".txt\"):\n",
        "            try:\n",
        "                myfile = open(file_path + filename, \"rt\")\n",
        "                contents = myfile.read()\n",
        "                fileTXT.append(contents)\n",
        "            except Exception:\n",
        "                print('Error reading .txt file: ' + filename)\n",
        "\n",
        "    return fileTXT\n",
        "\n",
        "# resumeTxt is a list containing the resumes of all the candidates.\n",
        "# Calling the function read_files to read all the resumes\n",
        "resumeTxt = read_files(resume_path)\n",
        "\n",
        "# Displaying the first resume\n",
        "resumeTxt[0]\n"
      ],
      "metadata": {
        "id": "WzHYG0KooVFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function read_files to read all the JDs\n",
        "jdTxt = read_files(jd_path)\n",
        "\n",
        "# Displaying the content of the first Job Description\n",
        "jdTxt[0]\n"
      ],
      "metadata": {
        "id": "s29xChULrA8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(Txt):\n",
        "    sw = stopwords.words('english')\n",
        "    space_pattern = '\\s+'\n",
        "    special_letters = \"[^a-zA-Z#]\"\n",
        "    p_txt = []\n",
        "\n",
        "    for resume in Txt:\n",
        "        text = re.sub(space_pattern, ' ', resume)  # Removes extra spaces\n",
        "        text = re.sub(special_letters, ' ', text)  # Removes special characters\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  # Removes punctuations\n",
        "        text = text.split()  # Splits the words in a text\n",
        "        text = [word for word in text if word.isalpha()]  # Keeps alphabetic words\n",
        "        text = [w for w in text if w not in sw]  # Removes stopwords\n",
        "        text = [item.lower() for item in text]  # Lowercases the words\n",
        "        p_txt.append(\" \".join(text))  # Joins all the words back\n",
        "\n",
        "    return p_txt\n"
      ],
      "metadata": {
        "id": "Wn1hfFyRtZqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function preprocessing to clean all the resumes\n",
        "p_resumeTxt = preprocessing(resumeTxt)\n",
        "\n",
        "# Displaying the first pre-processed resume\n",
        "p_resumeTxt[0]\n"
      ],
      "metadata": {
        "id": "Z0GTdaaHz81k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function preprocessing to clean all the job descriptions\n",
        "jds = preprocessing(jdTxt)\n",
        "\n",
        "# Displaying the first pre-processed job description\n",
        "jds[0]\n"
      ],
      "metadata": {
        "id": "h4uUnuS1yz1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining Resumes and Job Description for finding TF-IDF and Cosine Similarity\n",
        "TXT = p_resumeTxt + jds\n",
        "\n",
        "# Finding TF-IDF score of all the resumes and JDs.\n",
        "tv = TfidfVectorizer(max_df=0.85, min_df=10, ngram_range=(1,3))\n",
        "\n",
        "# Converting TF-IDF to a DataFrame\n",
        "tfidf_wm = tv.fit_transform(TXT)\n",
        "tfidf_tokens = tv.get_feature_names()\n",
        "df_tfidfvect1 = pd.DataFrame(data=tfidf_wm.toarray(), columns=tfidf_tokens)\n",
        "\n",
        "print(\"\\nTD-IDF Vectorizer\\n\")\n",
        "print(df_tfidfvect1[0:10])\n"
      ],
      "metadata": {
        "id": "0nnqBYlB0M1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Reduction"
      ],
      "metadata": {
        "id": "Nw7D7S3_0Q0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining transformation\n",
        "dimrec = TruncatedSVD(n_components=30, n_iter=7, random_state=42)\n",
        "transformed = dimrec.fit_transform(df_tfidfvect1)\n",
        "\n",
        "# Converting transformed vector to list\n",
        "vl = transformed.tolist()\n",
        "\n",
        "# Converting list to DataFrame\n",
        "fr = pd.DataFrame(vl)\n",
        "\n",
        "print('SVD Feature Vector')\n",
        "print(fr[0:10])\n"
      ],
      "metadata": {
        "id": "Q58lLdk50eYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building"
      ],
      "metadata": {
        "id": "fo5xFZCo0two"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Cosine Similarity between JDs and resumes to find out which resume is the best fit for a job description\n",
        "similarity = cosine_similarity(df_tfidfvect1[0:len(resumeTxt)], df_tfidfvect1[len(resumeTxt):])\n",
        "\n",
        "# Column names for job description\n",
        "abc = [f\"JD {i}\" for i in range(1, len(jds) + 1)]\n",
        "\n",
        "# DataFrame of similarity score\n",
        "Data = pd.DataFrame(similarity, columns=abc)\n",
        "\n",
        "print('\\nCosine Similarity\\n')\n",
        "print(Data[0:10])\n"
      ],
      "metadata": {
        "id": "6wo6v53q0wbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame of original resume\n",
        "t = pd.DataFrame({'Original Resume': resumeTxt})\n",
        "dt = pd.concat([Data, t], axis=1)  # Assuming a missing closing parenthesis after axis=1\n"
      ],
      "metadata": {
        "id": "41TK7pXK0nqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def number(text):\n",
        "    pattern = re.compile(r'([+(]?\\d+[)\\-]?[ \\t\\r\\f\\v]*[(]?\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*\\d*)')\n",
        "    pt = pattern.findall(text)\n",
        "    pt = [re.sub(r'[,.]', '', ah) for ah in pt if len(re.sub(r'[()\\-.,\\s+]', '', ah)) > 9]\n",
        "    pt = [re.sub(r'\\D$', '', ah).strip() for ah in pt]\n",
        "    pt = [ah for ah in pt if len(re.sub(r'\\D', '', ah)) <= 15]\n",
        "\n",
        "    for ah in list(pt):\n",
        "        if len(ah.split('-')) > 3:\n",
        "            continue\n",
        "        for x in ah.split(\"-\"):\n",
        "            try:\n",
        "                if x.strip()[-4:].isdigit():\n",
        "                    if int(x.strip()[-4:]) in range(1900, 2100):\n",
        "                        pt.remove(ah)\n",
        "            except: pass\n",
        "\n",
        "    numbers = list(set(pt))\n",
        "    return numbers\n"
      ],
      "metadata": {
        "id": "Qu85V4NT1_OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def email_ID(text):\n",
        "    # Compile helps us define a pattern for matching in the text\n",
        "    pattern = re.compile(r'[A-Za-z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+')\n",
        "    return pattern.findall(str(text))\n"
      ],
      "metadata": {
        "id": "DAT2LHXG2OHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the email_ID function to get the list of candidate’s e-mails\n",
        "dt['E-Mail ID'] = dt['Original Resume'].apply(lambda x: email_ID(x))\n",
        "\n",
        "print(\"Extracting e-mail from dataframe columns:\")\n",
        "print(dt['E-Mail ID'][0:5])\n"
      ],
      "metadata": {
        "id": "qVzdnItz2WlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def rm_number(text):\n",
        "    try:\n",
        "        pattern = re.compile(r'([+(]?\\d+[)\\-]?[ \\t\\r\\f\\v]*[(]?\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*\\d*)')\n",
        "        pt = pattern.findall(text)\n",
        "        pt = [re.sub(r'[,.]', '', ah) for ah in pt if len(re.sub(r'[()\\-.,\\s+]', '', ah)) > 9]\n",
        "        pt = [re.sub(r'\\D$', '', ah).strip() for ah in pt]\n",
        "        pt = [ah for ah in pt if len(re.sub(r'\\D', '', ah)) <= 15]\n",
        "\n",
        "        for ah in list(pt):\n",
        "            if len(ah.split('-')) > 3:\n",
        "                continue\n",
        "            for x in ah.split(\"-\"):\n",
        "                try:\n",
        "                    if x.strip()[-4:].isdigit():\n",
        "                        if int(x.strip()[-4:]) in range(1900, 2100):\n",
        "                            pt.remove(ah)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        number = pt\n",
        "        number = set(number)\n",
        "        number = list(number)\n",
        "\n",
        "        for i in number:\n",
        "            text = text.replace(i, \" \")\n",
        "\n",
        "        return text\n",
        "    except:\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "jE2FkAmO2t1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function rm_number to remove the phone number\n",
        "dt['Original'] = dt['Original Resume'].apply(lambda x: rm_number(x))\n"
      ],
      "metadata": {
        "id": "bWTYbT0L21iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def rm_email(text):\n",
        "    try:\n",
        "        email = None\n",
        "        # compile helps define a pattern for matching it in the text\n",
        "        pattern = re.compile('[\\w\\.-]+@[\\w\\.-]+')\n",
        "        # findall finds the pattern defined in compile\n",
        "        pt = pattern.findall(text)\n",
        "        email = pt\n",
        "        email = set(email)\n",
        "        email = list(email)\n",
        "\n",
        "        for i in email:\n",
        "            # replace will replace a given string with another\n",
        "            text = text.replace(i, \" \")\n",
        "\n",
        "        return text\n",
        "    except:\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "Hg2iAdPj2-vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function rm_email to remove the e-mails\n",
        "dt['Original'] = dt['Original'].apply(lambda x: rm_email(x))\n",
        "print(\"Extracting numbers from dataframe columns:\")\n",
        "print(dt['Original'][0:5])\n"
      ],
      "metadata": {
        "id": "OTAoNCZr2__x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove emails to extract the year of experience and name of a candidate\n",
        "def rm_email(text):\n",
        "    try:\n",
        "        email = None\n",
        "        # compile helps us to define a pattern for matching it in the text\n",
        "        pattern = re.compile('[\\w\\.-]+@[\\w\\.-]+')\n",
        "        # findall finds the pattern defined in compile\n",
        "        pt = pattern.findall(text)\n",
        "        email = pt\n",
        "        email = set(email)\n",
        "        email = list(email)\n",
        "        for i in email:\n",
        "            # replace will replace a given string with another\n",
        "            text = text.replace(i, \" \")\n",
        "        return text\n",
        "    except:\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "NTJIEgGbD9Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function rm_email to remove the e-mails\n",
        "dt['Original'] = dt['Original'].apply(lambda x: rm_email(x))\n",
        "print(\"Extracting numbers from dataframe columns:\")\n",
        "print(dt['Original'][0:5])\n"
      ],
      "metadata": {
        "id": "-nCtJej1Ffg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get candidates name\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def person_name(text):\n",
        "    # Tokenizes whole text to sentences\n",
        "    Sentences = nltk.sent_tokenize(text)\n",
        "    t = []\n",
        "    for s in Sentences:\n",
        "        # Tokenizes sentences to words\n",
        "        t.append(nltk.word_tokenize(s))\n",
        "    # Tags a word with its part of speech\n",
        "    words = [nltk.pos_tag(token) for token in t]\n",
        "    n = []\n",
        "    for x in words:\n",
        "        for l in x:\n",
        "            # match matches the pos tag of a word to a given tag here\n",
        "            if re.match('[NN.*]', l[1]):\n",
        "                n.append(l[0])\n",
        "    cands = []\n",
        "    for nouns in n:\n",
        "        if not wordnet.synsets(nouns):\n",
        "            cands.append(nouns)\n",
        "    cand = ' '.join(cands[:1])\n",
        "    return cand\n"
      ],
      "metadata": {
        "id": "Ly7HsMkfFlXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function name to extract the name of a candidate\n",
        "dt['Candidate\\'s Name'] = dt['Original'].apply(lambda x: person_name(x))\n",
        "print(\"Extracting names from dataframe columns:\")\n",
        "print(dt['Candidate\\'s Name'][0:5])\n"
      ],
      "metadata": {
        "id": "MERbmaS0J1ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find the years of experience\n",
        "def exp(text):\n",
        "    try:\n",
        "        e = []\n",
        "        p = 0\n",
        "        text = text.lower()\n",
        "\n",
        "        # Searches a pattern text string similar to the given pattern\n",
        "        pt1 = re.search(r\"(?:[a-zA-Z'-]+[^a-zA-Z'-]+){0,7}experience(?:[^a-zA-Z'-]+[a-zA-Z'-]+){0,7}\", text)\n",
        "        if pt1 is not None:\n",
        "            # groups all the string found in match\n",
        "            p = pt1.group()\n",
        "\n",
        "        # Searches a pattern text string similar to the given pattern\n",
        "        pt2 = re.search(r\"(?:[a-zA-Z'-]+[^a-zA-Z'-]+){0,2}year(?:[^a-zA-Z'-]+[a-zA-Z'-]+){0,2}\", text)\n",
        "        if pt2 is not None:\n",
        "            # groups all the string found in match\n",
        "            p = pt2.group()\n",
        "\n",
        "        # Searches a pattern text string similar to the given pattern\n",
        "        pt3 = re.search(r\"(?:[a-zA-Z'-]+[^a-zA-Z'-]+){0,2}years(?:[^a-zA-Z'-]+[a-zA-Z'-]+){0,2}\", text)\n",
        "        if pt3 is not None:\n",
        "            # groups all the string found in match\n",
        "            p = pt3.group()\n",
        "\n",
        "        if p == 0:\n",
        "            return 0\n",
        "\n",
        "        # findall finds the pattern defined in compile\n",
        "        ep = re.findall('[0-9]{1,2}', p)\n",
        "        ep_int = list(map(int, ep))\n",
        "\n",
        "        # this for loop is for filtering and then appending string containing years of experience\n",
        "        for a in ep:\n",
        "            for b in ep_int:\n",
        "                if len(a) <= 2 and b < 30:\n",
        "                    e.append(a)\n",
        "\n",
        "        ep = ''.join(e[0])\n",
        "\n",
        "        # findall finds the pattern defined in compile\n",
        "        p1 = re.findall('[0-9]{1,2}.[0-9]{1,2}', p)\n",
        "        exp = []\n",
        "        if not p1:\n",
        "            exp.append(ep)\n",
        "            exp = ''.join(ep)\n",
        "        else:\n",
        "            exp.append(p1)\n",
        "            exp = ''.join(p1)\n",
        "    except:\n",
        "        exp = 0\n",
        "\n",
        "    return exp\n"
      ],
      "metadata": {
        "id": "aJdS60PJKe7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function exp to extract the year of experience of the candidate\n",
        "dt['Experience'] = dt['Original'].apply(lambda x: exp(x))\n",
        "print(\"Extracting e-mail from dataframe columns:\")\n",
        "print(dt['Experience'])\n"
      ],
      "metadata": {
        "id": "coo62vYVSWUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing a file of pre-defined skills\n",
        "skills = pd.read_excel('/content/drive/MyDrive/skills.xlsx')\n",
        "skills = skills.values.flatten().tolist()\n",
        "\n",
        "i = 0\n",
        "skill = []\n",
        "\n",
        "for z in skills:\n",
        "    r = z.lower()\n",
        "    skill.append(r)\n",
        "    i += 1\n"
      ],
      "metadata": {
        "id": "bel_zapDSbCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract skills from candidate's resume\n",
        "def skills(text):\n",
        "    sw = set(nltk.corpus.stopwords.words('english'))\n",
        "    tokens = nltk.tokenize.word_tokenize(text)\n",
        "\n",
        "    # Remove the punctuation\n",
        "    ft = [w for w in tokens if w.isalpha()]\n",
        "\n",
        "    # Remove the stop words\n",
        "    ft = [w for w in tokens if w not in sw]\n",
        "\n",
        "    # Generate bigrams and trigrams (like Machine Learning)\n",
        "    n_grams = list(map(' '.join, nltk.everygrams(ft, 2, 3)))\n",
        "\n",
        "    fs = set()\n",
        "\n",
        "    # Check for each token in our skills database\n",
        "    for token in ft:\n",
        "        if token.lower() in skill:\n",
        "            fs.add(token)\n",
        "\n",
        "    # Check for each bigram and trigram in our skills database\n",
        "    for ngram in n_grams:\n",
        "        if ngram.lower() in skill:\n",
        "            fs.add(ngram)\n",
        "\n",
        "    return fs\n"
      ],
      "metadata": {
        "id": "x3WpvOLrZsdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function skills to extract the skills of a candidate\n",
        "dt['Skills'] = dt['Original'].apply(lambda x: skills(x))\n",
        "print(\"Extracting Person Name from dataframe columns:\")\n",
        "print(dt['Skills'])\n"
      ],
      "metadata": {
        "id": "yo37bUsSZztU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract Location\n",
        "def location(text):\n",
        "    place_entity = locationtagger.find_locations(text=text)\n",
        "    return place_entity.cities\n"
      ],
      "metadata": {
        "id": "5kHGNzREarbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function location to extract the location of a candidate\n",
        "dt['Location'] = dt['Resume'].apply(lambda x: location(x))\n",
        "print(\"Extracting cities from dataframe columns:\")\n",
        "print(dt['Location'])\n"
      ],
      "metadata": {
        "id": "IgHkZ8pcbBwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract Company name\n",
        "def CompanyName(text):\n",
        "    # for tagging each entity with its labels\n",
        "    tokens = nlp(str(text))\n",
        "    x = []\n",
        "\n",
        "    # for loop for extracting company names\n",
        "    for ent in tokens.ents:\n",
        "        if ent.label_ == 'ORG':\n",
        "            return ent.text\n"
      ],
      "metadata": {
        "id": "IL1bbqxdbZ_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function CompanyName to extract past companies of a candidate\n",
        "dt['Company Name'] = dt['Original Resume'].apply(lambda x: CompanyName(x))\n",
        "print(\"Extracting Person Name from dataframe columns:\")\n",
        "print(dt['Company Name'])\n"
      ],
      "metadata": {
        "id": "m-2R0zbFbzAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final result for Project Manager profile\n",
        "pm = dt[['Project Manager', 'Candidate\\'s Name', 'Phone No.', 'E-Mail ID', 'Skills', 'Experience', 'Location', 'Company Name']]\n",
        "pm = pm.sort_values(by='Project Manager', ascending=False)\n",
        "print(pm[0:10])\n"
      ],
      "metadata": {
        "id": "NE_IupshcVmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Result for Business Analyst\n",
        "ba = dt[['Business Analyst', 'Candidate\\'s Name', 'Phone No.', 'E-Mail ID', 'Skills', 'Experience', 'Location', 'Company Name']]\n",
        "ba = ba.sort_values(by='Business Analyst', ascending=False)\n",
        "print(ba[0:10])\n"
      ],
      "metadata": {
        "id": "WzWx_K37cl2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Result for Java Developer\n",
        "jad = dt[['Java Developer', 'Candidate\\'s Name', 'Phone No.', 'E-Mail ID', 'Skills', 'Experience', 'Location', 'Company Name']]\n",
        "jad = jad.sort_values(by='Java Developer', ascending=False)\n",
        "print(jad[0:10])\n"
      ],
      "metadata": {
        "id": "HuIiLuVudCgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "D6n0KRdOfoF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create and generate a word cloud image for the best candidate for Project Manager\n",
        "wordcloud = WordCloud(width=800, height=500, background_color='white', min_font_size=10).generate(resumeTxt[17])\n",
        "\n",
        "# Display the generated image\n",
        "plt.figure(figsize=(20, 5), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "47zewzUanh_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create and generate a word cloud image for the best candidate for Business Analyst\n",
        "wordcloud = WordCloud(width=800, height=500, background_color='white', min_font_size=10).generate(resumeTxt[12])\n",
        "\n",
        "# Display the generated image\n",
        "plt.figure(figsize=(20, 5), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "10dNICKBoaB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create and generate a word cloud image for the best candidate for Java Developer\n",
        "wordcloud = WordCloud(width=800, height=500, background_color='white', min_font_size=10).generate(resumeTxt[23])\n",
        "\n",
        "# Display the generated image\n",
        "plt.figure(figsize=(20, 5), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5REmUVFmqLRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create and generate a word cloud of the job description for a Java Developer\n",
        "wordcloud = WordCloud(width=800, height=500, background_color='white', min_font_size=10).generate(jds[0])\n",
        "\n",
        "# Display the generated image\n",
        "plt.figure(figsize=(20, 5), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IgbP9ZNJqNjz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}